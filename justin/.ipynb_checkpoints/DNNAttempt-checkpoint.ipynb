{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Name: /physical_device:GPU:0   Type: GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "print(tf.__version__)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n",
    "    \n",
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pkSeqID proto            saddr  sport          daddr dport     seq  \\\n",
      "0  3142762   udp  192.168.100.150   6551  192.168.100.3    80  251984   \n",
      "1  2432264   tcp  192.168.100.150   5532  192.168.100.3    80  256724   \n",
      "2  1976315   tcp  192.168.100.147  27165  192.168.100.3    80   62921   \n",
      "3  1240757   udp  192.168.100.150  48719  192.168.100.3    80   99168   \n",
      "4  3257991   udp  192.168.100.147  22461  192.168.100.3    80  105063   \n",
      "\n",
      "     stddev  N_IN_Conn_P_SrcIP       min  state_number      mean  \\\n",
      "0  1.900363                100  0.000000             4  2.687519   \n",
      "1  0.078003                 38  3.856930             3  3.934927   \n",
      "2  0.268666                100  2.974100             3  3.341429   \n",
      "3  1.823185                 63  0.000000             4  3.222832   \n",
      "4  0.822418                100  2.979995             4  3.983222   \n",
      "\n",
      "   N_IN_Conn_P_DstIP  drate     srate       max  attack category subcategory  \n",
      "0                100    0.0  0.494549  4.031619       1     DDoS         UDP  \n",
      "1                100    0.0  0.256493  4.012924       1     DDoS         TCP  \n",
      "2                100    0.0  0.294880  3.609205       1     DDoS         TCP  \n",
      "3                 63    0.0  0.461435  4.942302       1      DoS         UDP  \n",
      "4                100    0.0  1.002999  4.994452       1     DDoS         UDP  \n",
      "(2934817, 19)\n",
      "Index(['pkSeqID', 'proto', 'saddr', 'sport', 'daddr', 'dport', 'seq', 'stddev',\n",
      "       'N_IN_Conn_P_SrcIP', 'min', 'state_number', 'mean', 'N_IN_Conn_P_DstIP',\n",
      "       'drate', 'srate', 'max', 'attack', 'category', 'subcategory'],\n",
      "      dtype='object')\n",
      "   pkSeqID proto            saddr  sport          daddr dport     seq  \\\n",
      "0   792371   udp  192.168.100.150  48516  192.168.100.3    80  175094   \n",
      "1  2056418   tcp  192.168.100.148  22267  192.168.100.3    80  143024   \n",
      "2  2795650   udp  192.168.100.149  28629  192.168.100.3    80  167033   \n",
      "3  2118009   tcp  192.168.100.148  42142  192.168.100.3    80  204615   \n",
      "4   303688   tcp  192.168.100.149   1645  192.168.100.5    80   40058   \n",
      "\n",
      "     stddev  N_IN_Conn_P_SrcIP       min  state_number      mean  \\\n",
      "0  0.226784                100  4.100436             4  4.457383   \n",
      "1  0.451998                100  3.439257             1  3.806172   \n",
      "2  1.931553                 73  0.000000             4  2.731204   \n",
      "3  0.428798                 56  3.271411             1  3.626428   \n",
      "4  2.058381                100  0.000000             3  1.188407   \n",
      "\n",
      "   N_IN_Conn_P_DstIP     drate     srate       max  attack category  \\\n",
      "0                100  0.000000  0.404711  4.719438       1      DoS   \n",
      "1                100  0.225077  0.401397  4.442930       1     DDoS   \n",
      "2                100  0.000000  0.407287  4.138455       1     DDoS   \n",
      "3                100  0.000000  0.343654  4.229700       1     DDoS   \n",
      "4                100  0.000000  0.135842  4.753628       1      DoS   \n",
      "\n",
      "  subcategory  \n",
      "0         UDP  \n",
      "1         TCP  \n",
      "2         UDP  \n",
      "3         TCP  \n",
      "4         TCP  \n",
      "(733705, 19)\n",
      "Index(['pkSeqID', 'proto', 'saddr', 'sport', 'daddr', 'dport', 'seq', 'stddev',\n",
      "       'N_IN_Conn_P_SrcIP', 'min', 'state_number', 'mean', 'N_IN_Conn_P_DstIP',\n",
      "       'drate', 'srate', 'max', 'attack', 'category', 'subcategory'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# feature_labels = np.genfromtxt('UNSW_2018_IoT_Botnet_Dataset_Feature_Names.csv', delimiter=',')\n",
    "# train_file_path = tf.keras.utils.get_file(\"UNSW_2018_IoT_Botnet_Dataset_Feature_Names.csv\", )\n",
    "# data = np.genfromtxt('UNSW_2018_IoT_Botnet_Dataset_62.csv', delimiter=',')\n",
    "\n",
    "base_dir = 'G:/Deep Learning Project/'    \n",
    "\n",
    "train_df = pd.read_csv(base_dir + \"UNSW_2018_IoT_Botnet_Final_10_best_Training.csv\", sep=',')\n",
    "test_df = pd.read_csv(base_dir + \"UNSW_2018_IoT_Botnet_Final_10_best_Testing.csv\", sep=',')\n",
    "\n",
    "\n",
    "# print(labels)\n",
    "print(train_df.head())\n",
    "print(train_df.shape)\n",
    "print(train_df.columns)\n",
    "\n",
    "print(test_df.head())\n",
    "print(test_df.shape)\n",
    "print(test_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Drop Seq ID\n",
    "# frame = frame.drop(['pkSeqID'], axis = 1)\n",
    "# # feature_labels.pop('pkSeqId')\n",
    "\n",
    "# #Drop Ips\n",
    "# frame = frame.drop(['saddr', 'daddr'], axis = 1)\n",
    "\n",
    "# #Drop blank fields\n",
    "# frame = frame.drop(['smac', 'dmac', 'soui', 'doui', 'sco', 'dco'], axis = 1)\n",
    "\n",
    "# #Drop flags and ports because they contain NaNs\n",
    "# frame = frame.drop(['flgs', 'sport', 'dport'], axis = 1)\n",
    "\n",
    "# print(frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode protocol\n",
    "\n",
    "# frame = pd.concat([frame ,pd.get_dummies(frame['proto'], prefix='proto',dummy_na=False)],axis=1).drop(['proto'],axis=1)\n",
    "# print(frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode state\n",
    "\n",
    "# frame = pd.concat([frame ,pd.get_dummies(frame['state'], prefix='state',dummy_na=False)],axis=1).drop(['state'],axis=1)\n",
    "# print(frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode category, skip if using pd.Categorical\n",
    "\n",
    "# frame = pd.concat([frame ,pd.get_dummies(frame['category'], prefix='category',dummy_na=False)],axis=1).drop(['category'],axis=1)\n",
    "\n",
    "# print(frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the target the 'attack' column (binary classification)\n",
    "# def label_target(row):\n",
    "#     if row['attack'] == 1:\n",
    "#         return 1\n",
    "#     return 0\n",
    "\n",
    "# frame['target'] = frame.apply(lambda row: label_target(row), axis = 1)\n",
    "# frame = frame.drop(['attack'], axis = 1)\n",
    "\n",
    "#Make the tartget column a categorical version of the 'category' column (classification)\n",
    "\n",
    "# frame['target'] = pd.Categorical(frame['category'])\n",
    "\n",
    "# print(\"Categories:\\n\")\n",
    "# for i,cat in enumerate(frame['target'].cat.categories):\n",
    "#     print(i, cat)\n",
    "\n",
    "# frame['target'] = frame.target.cat.codes.astype(int)\n",
    "\n",
    "# frame = frame.drop(['category'], axis = 1)\n",
    "\n",
    "#Remove the encoded categories and keep the target\n",
    "# frame = frame[frame.columns.drop(list(frame.filter(regex='category_')))]\n",
    "\n",
    "# print(frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode subcategory\n",
    "\n",
    "# frame = pd.concat([frame ,pd.get_dummies(frame['subcategory'], prefix='subcategory',dummy_na=False)],axis=1).drop(['subcategory'],axis=1)\n",
    "\n",
    "\n",
    "#Remove subcategory\n",
    "# frame = frame.drop(['subcategory'], axis = 1)\n",
    "\n",
    "# print(frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize \n",
    "\n",
    "# cols_to_normalize = ['stime', 'ltime', 'dur', 'mean', 'stddev',\n",
    "#        'sum', 'min', 'max', 'rate', 'srate', 'drate']\n",
    "\n",
    "# cols_to_normalize = frame.columns\n",
    "\n",
    "# print(frame.dtypes)\n",
    "\n",
    "#Mean min-max\n",
    "#frame=(df-df.min())/(df.max()-df.min())\n",
    "# frame[cols_to_normalize] = frame[cols_to_normalize].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "#Mean normalization\n",
    "# normalized_df=(df-df.mean())/df.std()\n",
    "\n",
    "# normalized_frame = (frame-frame.min())/(frame.max()-frame.min())\n",
    "# frame[cols_to_normalize] = frame[cols_to_normalize].apply(lambda x: (x - x.mean()) / x.std()))\n",
    "\n",
    "# print(frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame = frame.astype('float64')\n",
    "\n",
    "# print(frame.dtypes)\n",
    "\n",
    "# print(frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2934817, 19)\n",
      "(2934817,)\n",
      "(733705, 19)\n",
      "(733705,)\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df.to_numpy()\n",
    "Y_train = np.asarray([np.asarray(x) for x in train_df[\"attack\"]]).astype(np.float32)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "X_test = test_df.to_numpy()\n",
    "Y_test = np.asarray([np.asarray(x) for x in test_df[\"attack\"]]).astype(np.float32)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          64000     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, None, 256)         247296    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 128)               49280     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 361,866\n",
      "Trainable params: 361,866\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
    "\n",
    "# The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\n",
    "model.add(layers.GRU(256, return_sequences=True))\n",
    "\n",
    "# The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n",
    "model.add(layers.SimpleRNN(128))\n",
    "\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "# Each MNIST image batch is a tensor of shape (batch_size, 28, 28).\n",
    "# Each input sequence will be of size (28, 28) (height is treated like time).\n",
    "input_dim = 10\n",
    "\n",
    "units = 64\n",
    "output_size = 2  # labels are attack or none\n",
    "\n",
    "# Build the RNN model\n",
    "def build_model(allow_cudnn_kernel=True):\n",
    "  # CuDNN is only available at the layer level, and not at the cell level.\n",
    "  # This means `LSTM(units)` will use the CuDNN kernel,\n",
    "  # while RNN(LSTMCell(units)) will run on non-CuDNN kernel.\n",
    "  if allow_cudnn_kernel:\n",
    "    # The LSTM layer with default options uses CuDNN.\n",
    "    lstm_layer = tf.keras.layers.LSTM(units, input_shape=(None, input_dim))\n",
    "  else:\n",
    "    # Wrapping a LSTMCell in a RNN layer will not use CuDNN.\n",
    "    lstm_layer = tf.keras.layers.RNN(\n",
    "        tf.keras.layers.LSTMCell(units),\n",
    "        input_shape=(None, input_dim))\n",
    "  model = tf.keras.models.Sequential([\n",
    "      lstm_layer,\n",
    "      tf.keras.layers.BatchNormalization(),\n",
    "      tf.keras.layers.Dense(output_size)]\n",
    "  )\n",
    "  return model\n",
    "\n",
    "model = build_model(allow_cudnn_kernel=True)\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_3_input to have 3 dimensions, but got array with shape (2934817, 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-1f8d3ba4b57f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m           epochs=5)\n\u001b[0m",
      "\u001b[1;32mg:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mg:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[1;32m--> 646\u001b[1;33m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[0;32m    647\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2383\u001b[1;33m         batch_size=batch_size)\n\u001b[0m\u001b[0;32m   2384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[1;32mg:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2408\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2409\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2410\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2412\u001b[0m     \u001b[1;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    571\u001b[0m                            \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    574\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected lstm_3_input to have 3 dimensions, but got array with shape (2934817, 19)"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          batch_size=batch_size,\n",
    "          epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
